{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c63aea3-078c-42ff-b296-8e20ebd4d360",
   "metadata": {},
   "source": [
    "- 레스토랑 리뷰 감성 분류하기 :\n",
    "https://github.com/rickiepark/nlp-with-pytorch/blob/main/chapter_3/3_5_Classifying_Yelp_Review_Sentiment.ipynb\n",
    "\n",
    "- NLP using GloVe Embeddings(FAKE NEWS) : \n",
    "https://www.kaggle.com/code/madz2000/nlp-using-glove-embeddings-99-87-accuracy\n",
    "- https://www.kaggle.com/code/lorwohl/fake-news-detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad238e88-41cc-44dd-bbe2-7a858ad24309",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모듈 로딩\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "from string import punctuation\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "# from nltk.stem import WordNetLemmatizer # 표제어 추출\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer # used to tokenize text sequences\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences # padding sequences to the same length\n",
    "from tensorflow.keras.models import Sequential # building sequential models like FF layers in the transformer encoder\n",
    "from tensorflow.keras.layers import Dense, Embedding, LSTM, GRU, SimpleRNN, Dropout # used for parts of the transformer encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df23a0ac-cc8a-445d-9157-1740fe407a02",
   "metadata": {},
   "source": [
    "---\n",
    "### [1] 데이터 보기 및 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69b6f28-801e-4d60-b150-0f173ab88c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "real = pd.read_csv(\"./True.csv\")\n",
    "fake = pd.read_csv(\"./Fake.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f56c9c0-b38f-4eb4-9a93-d1048537c9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "real.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f64491-f75e-4de0-924d-61a33ed73b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdaf534c-e30b-4447-83f2-4b31c0f8f570",
   "metadata": {},
   "outputs": [],
   "source": [
    "# real / fake에 범주 부여\n",
    "fake['category']=0\n",
    "real['category']=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee7803b-aff6-4e1d-a57c-224fb7c74aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# real / fake 합치기\n",
    "df=pd.concat([real, fake])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa64f3e0-2b2c-474d-9b29-fc4f43a7ff18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 카테고리 확인\n",
    "print(df.category.value_counts())\n",
    "print(f'RealNEws : {round(df.category.value_counts()[0] / df.category.count(), 2)}%')\n",
    "print(f'FakeNews : {round(df.category.value_counts()[1] / df.category.count(), 2)}%')\n",
    "\n",
    "# sns.countplot => 갯수 확인 시각화\n",
    "sns.countplot(df.category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0c44fc-571c-4bac-8c41-fab239117768",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 중복 제거\n",
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4bb8cb7-f5d6-4d5b-a26f-976c56098916",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정보 확인\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7132b885-3148-4cb8-86e9-64ee1e9b1af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결측치 확인\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a011c2-8d06-4e18-bd29-e11845fe9936",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 형태 확인\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0158ef73-d965-4660-ba94-7ef9be9efdb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 뉴스의 종류 확인\n",
    "print(df.subject.value_counts())\n",
    "\n",
    "# 카테고리 별 뉴스 종류 확인\n",
    "plt.xticks(rotation=70)\n",
    "sns.countplot(x='subject', hue='category', data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69770c9-ab39-4c87-98d6-2612e4d018b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Real News\n",
    "plt.figure(figsize = (20,20))\n",
    "wc = WordCloud(max_words = 2000 , width = 1600 , height = 800 , stopwords = STOPWORDS).generate(\" \".join(df[df.category == 1].text))\n",
    "plt.imshow(wc , interpolation = 'bilinear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df372d93-d827-48ed-b69e-49eb63c9b105",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Fake News\n",
    "plt.figure(figsize = (20,20))\n",
    "wc = WordCloud(max_words = 2000 , width = 1600 , height = 800 , stopwords = STOPWORDS).generate(\" \".join(df[df.category == 0].text))\n",
    "plt.imshow(wc , interpolation = 'bilinear')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8465346-dcf4-425a-8a04-4766f274d57b",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "## [2] 데이터 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca200308-beb0-4614-99c0-e62230711d81",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 뉴스 기사들을 한 컬럼으로 합치기\n",
    "df['text_all']=df['text'] + ' ' + df['title']\n",
    "df['text_all'].head()\n",
    "\n",
    "del df['title']\n",
    "del df['subject']\n",
    "del df['date']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef5baba-4169-4cfe-aade-81b9be72419d",
   "metadata": {},
   "source": [
    "### 데이터 분할"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4cebaa-c13a-417f-9210-c27ff6135516",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split, 비율에 맞게 분할\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.text_all, \n",
    "                                                    df.category, \n",
    "                                                    test_size=0.2, \n",
    "                                                    stratify=df.category,\n",
    "                                                    random_state=11)\n",
    "\n",
    "print(f'X_train : {X_train.shape}, X_test : {X_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1858fc-0a25-411a-afb9-f3b426b29e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X_train, \n",
    "                                                  y_train, \n",
    "                                                  test_size=0.2, \n",
    "                                                  stratify=y_train,\n",
    "                                                  random_state=11)\n",
    "print(f'X_train : {X_train.shape}, X_test : {X_test.shape} , X_val : {X_val.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5dceb0a-c49c-47d2-b35a-4fd9a8d8fb71",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n', '학습 데이터', '-'*20)\n",
    "print(f'X_train : {X_train.shape}, y_train : {y_train.shape}')\n",
    "print(f'가짜뉴스 : {round(y_train.value_counts()[0]/len(y_train),2)}%')\n",
    "print(f'진짜뉴스 : {round(y_train.value_counts()[1]/len(y_train),2)}%')\n",
    "\n",
    "print('\\n', '테스트 데이터', '-'*20)\n",
    "print(f'X_test   : {X_test.shape}, y_test : {y_test.shape}')\n",
    "print(f'가짜뉴스 : {round(y_test.value_counts()[0]/len(y_test),2)}%')\n",
    "print(f'진짜뉴스 : {round(y_test.value_counts()[1]/len(y_test),2)}%')\n",
    "\n",
    "print('\\n', '검증 데이터', '-'*20)\n",
    "print(f'X_val    : {X_val.shape}, y_val : {y_val.shape}')\n",
    "print(f'가짜뉴스 : {round(y_val.value_counts()[0]/len(y_val),2)}%')\n",
    "print(f'진짜뉴스 : {round(y_val.value_counts()[1]/len(y_val),2)}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2bacdd-c967-42ed-b410-a737e1bdc5ef",
   "metadata": {},
   "source": [
    "### 불용어 처리\n",
    "string.punctuation -> !\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ae8098-ef15-4b5a-960f-cad1f36c73f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS # wordcloud 모듈에도 stopword 기능 있음?\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop=set(stopwords.words('english'))\n",
    "# stopwords=set(nltk.corpus.stopwords.words('english'))\n",
    "punctuation = string.punctuation\n",
    "stop.update(punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3904dcb0-4d8a-44f7-a8ea-96d595434f32",
   "metadata": {},
   "source": [
    "### 텍스트 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e4d7ec-8152-4741-b0f1-72092c78c9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텍스트 처리하는 함수\n",
    "def textProcess(textData):\n",
    "    refined_texts = []\n",
    "    # wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    w_tokens = word_tokenize(textData) # 단어_토큰화\n",
    "    for w in w_tokens:\n",
    "        \n",
    "        if w not in stop: # english stopwords에 포함 안되어있다면 ~ 해라\n",
    "            # string.puctuation 안했으면 refined_T = re.sub('[^a-zA-Z]', '', w)\n",
    "            refined_t = w.lower() # 소문자로\n",
    "            \n",
    "            # 표제어 추출\n",
    "            # refined_t = wordnet_lemmatizer.lemmatize(w)\n",
    "            refined_texts.append(refined_t)\n",
    "    \n",
    "    return \" \".join(refined_texts) # 단어에서 문장으로 복원"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa7c5e5-25ee-4039-92df-5cc5399faa63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# refined_data = []\n",
    "# for i in df.text_all:\n",
    "#     refined_data.append(textProcess(i))\n",
    "    \n",
    "X_train = [textProcess(i) for i in X_train]\n",
    "X_test = [textProcess(i) for i in X_test]\n",
    "X_val = [textProcess(i) for i in X_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dbee253-9e1c-4d57-9440-8637d206f14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352a5855-bfea-4961-9b40-3fd2a6d20aa5",
   "metadata": {},
   "source": [
    "### 텍스트 데이터 토큰화 및 수치화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb3bfa0-d62c-4376-96f1-481b7b25ee17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텍스트 데이터 토큰화한 후 수치화\n",
    "def makeToken(textData, numWord=0):\n",
    "    if numWord>0:\n",
    "        myToken=Tokenizer(num_words=numWord)\n",
    "    else:\n",
    "        myToken=Tokenizer()\n",
    "    \n",
    "    # 단어사전(voca) 생성\n",
    "    myToken.fit_on_texts(textData)\n",
    "    seq_Token = myToken.texts_to_sequences(textData)\n",
    "    \n",
    "    # voca 총개수\n",
    "    voca_w_num = len(myToken.word_index)\n",
    "    \n",
    "    return (seq_Token, voca_w_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7824e7b-0130-4122-ab9d-5dccdf26f569",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_train_vocaNum = makeToken(X_train)\n",
    "X_test, X_test_vocaNum = makeToken(X_test)\n",
    "X_val, X_val_vocaNum = makeToken(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4fb415-96fb-479f-9884-5c7fd4e81b45",
   "metadata": {},
   "source": [
    "### 단어 갯수 파악"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4efe90-0b7d-48ce-884b-29c3737c0c39",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 단어 갯수 파악 함수\n",
    "def checkLength(datas):\n",
    "\n",
    "  # 기사 개당 단어 개수\n",
    "  length=[len(data) for data in datas]\n",
    "  \n",
    "  # 히스토그램\n",
    "  plt.figure(figsize=(12,8))\n",
    "  plt.hist(length)\n",
    "  plt.title(f'Max {max(length)}  Min {min(length)}  AVG {round(sum(length)/len(length),2)}')\n",
    "  plt.xlabel('data length')\n",
    "  plt.ylabel('data number')\n",
    "  plt.show()\n",
    "# return length\n",
    "  \n",
    "checkLength(X_train)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf0b872-c95c-4261-a8b4-7f24de0c09c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig,(ax1,ax2)=plt.subplots(1,2,figsize=(12,8))\n",
    "\n",
    "# text_len=df[df['category']==1]['text_all'].str.len()\n",
    "# ax1.hist(text_len,color='red')\n",
    "# ax1.set_title('Real text')\n",
    "\n",
    "# text_len=df[df['category']==0]['text_all'].str.len()\n",
    "# ax2.hist(text_len,color='green')\n",
    "# ax2.set_title('Fake text')\n",
    "\n",
    "# fig.suptitle('Characters in texts')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f552cf-2af5-41e2-9444-3d08f562b30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig,(ax1,ax2)=plt.subplots(1,2,figsize=(12,8))\n",
    "# text_len=df[df['category']==1]['text_all'].str.split().map(lambda x: len(x))\n",
    "# ax1.hist(text_len,color='red')\n",
    "# ax1.set_title('Real text')\n",
    "# text_len=df[df['category']==0]['text_all'].str.split().map(lambda x: len(x))\n",
    "# ax2.hist(text_len,color='green')\n",
    "# ax2.set_title('Fake text')\n",
    "# fig.suptitle('Words in texts')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b220beae-efa5-416f-b54f-51557413df6b",
   "metadata": {},
   "source": [
    "### pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726cd792-891b-48cf-ad5a-cdfc26a59e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 300\n",
    "X_train = pad_sequences(X_train, maxlen=maxlen)\n",
    "X_test = pad_sequences(X_test, maxlen=maxlen)\n",
    "X_val = pad_sequences(X_val, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b353c6-dce5-4910-8d5c-649c71b0c4e2",
   "metadata": {},
   "source": [
    "---\n",
    "## 모델 구성\n",
    "https://jimmy-ai.tistory.com/281"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc77ab21-1c10-4849-bca3-577c71f20a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMB_DIM = 32                     # 임베딩 벡터 크기 \n",
    "WORD_NUM = X_train_vocaNum         # 단어사전 수\n",
    "HIDDEN_NODE = 64                # 은닉층 뉴런 수\n",
    "INPUT_LENGTH = 300         # 1문장의 토큰 수 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f874226e-e30d-4687-8bed-3522379891e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "# RNN 적용을 위한 임베딩 지정\n",
    "model.add(Embedding(WORD_NUM, EMB_DIM, input_length=INPUT_LENGH))\n",
    "\n",
    "### RNN 파트 시작점 ###\n",
    "\n",
    "# 이중층 GRU -> SimpleRNN, 단일 방향 예시\n",
    "model.add(GRU(HIDDEN_NODE, return_sequences=True))\n",
    "model.add(SimpleRNN(HIDDEN_NODE))\n",
    "\n",
    "### RNN 파트 끝점 ###\n",
    "\n",
    "# fc layer 부분(32 차원 변환 -> dropout -> 이진 분류 결과)\n",
    "model.add(Dense(32, activation = 'relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation = 'sigmoid')) # 이진 분류를 위한 마지막 layer 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432f1be6-9a1d-4797-8b53-1cf66cdb27d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b813004-5f17-44f4-a1de-160bed64161a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90df8c8-4428-42f3-8cf3-818c33d9acbd",
   "metadata": {},
   "source": [
    "## 모델 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e101b19-cefa-45a9-a927-089d68925737",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0105fb-0f0b-4d51-92cb-2695f74ec0c5",
   "metadata": {},
   "source": [
    "## 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5884978d-386d-4762-885e-a202a4c26570",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8066461e-5726-4a07-a2de-d632fb5dc64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate_reduction = ReduceLROnPlateau(monitor='val_accuracy', \n",
    "                                            patience = 2, \n",
    "                                            verbose=1,\n",
    "                                            factor=0.5, \n",
    "                                            min_lr=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357e4665-bf49-49d9-8444-9ca14da42e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train, epochs = epochs, \n",
    "                    batch_size = batch_size, validation_data = (X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f510f27-498c-4f5d-ab6f-295cb2b63386",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f09266f-62cd-4d48-b1ec-394ee7ac2c3d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
