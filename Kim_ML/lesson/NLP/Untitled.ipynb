{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e758e10c-fd1e-4a07-b667-0057c7dc630f",
   "metadata": {},
   "source": [
    "# 텍스트 전처리\n",
    "- 패키지 설치\n",
    "    - NLTK : pip install nltk\n",
    "    - KoNLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23e73455-17b2-4921-878f-c9daa469c8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install --user --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55637c55-d927-4f16-b9c8-d3adc03c45f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ef4af37-d1f2-4c08-a21b-dedde76b26d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install konlpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6181b8d2-184c-4c90-89d7-b67e3b27dd61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk LookupError\n",
    "import nltk\n",
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3fe31d-0bb7-41b6-9688-b3161c55d48d",
   "metadata": {},
   "source": [
    "##  [1] 토큰화(Tokenize)\n",
    "- 문장/ 문서를 의미를 지닌 작은 단위로 나뉘는 것\n",
    "- 나누어진 단어를 토큰(Token)이라 함.\n",
    "- 종류\n",
    "    - 문장 토큰화\n",
    "    - 단어 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "aa93c647-2029-424d-8102-4b6e57cb9bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a3c361a0-fe6f-4fce-9656-27be55207e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text_01='Caution: when tokenizing a Unicode string,\\\n",
    "make sure you are not using an encoded version of the string (it may be necessary to decode it first,\\\n",
    "e.g. with s.decode(\"utf8\").'\n",
    "\n",
    "raw_text_02=\"\"\"Return a tokenized copy of text, using NLTK’s recommended word tokenizer (currently an improved TreebankWordTokenizer along with PunktSentenceTokenizer for the specified language).\n",
    "\"\"\"\n",
    "\n",
    "raw_text_03=\"This particular tokenizer requires the Punkt sentence tokenization models to be installed.\\\n",
    "NLTK also provides a simpler, regular-expression based tokenizer, which splits text on whitespace and punctuation:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "bf65f988-c0a9-4f87-b92c-b2a7b3895b1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Caution', ':', 'when', 'tokenizing', 'a', 'Unicode', 'string', ',', 'make', 'sure', 'you', 'are', 'not', 'using', 'an', 'encoded', 'version', 'of', 'the', 'string', '(', 'it', 'may', 'be', 'necessary', 'to', 'decode', 'it', 'first', ',', 'e.g', '.', 'with', 's.decode', '(', '``', 'utf8', \"''\", ')', '.']\n"
     ]
    }
   ],
   "source": [
    "# 단어 단위 토큰화 word_tokenize\n",
    "result_01=word_tokenize(raw_text_01)\n",
    "print(result_01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "720cb553-dc26-4f18-895d-176c0f6562cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Caution: when tokenizing a Unicode string, make sure you are not using an encoded version of the string (it may be necessary to decode it first, e.g. with s.decode(\"utf8\").', 'Return a tokenized copy of text, using NLTK’s recommended word tokenizer (currently an improved TreebankWordTokenizer along with PunktSentenceTokenizer for the specified language).\\n', 'This particular tokenizer requires the Punkt sentence tokenization models to be installed.NLTK also provides a simpler, regular-expression based tokenizer, which splits text on whitespace and punctuation:']\n"
     ]
    }
   ],
   "source": [
    "raw_text_list=[raw_text01, raw_text_02, raw_text_03]\n",
    "print(raw_text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "67ce8232-27ee-4181-a7d5-26c54dc699fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Caution: when tokenizing a Unicode string,make sure you are not using an encoded version of the string (it may be necessary to decode it first,e.g.', 'with s.decode(\"utf8\").'] 2\n"
     ]
    }
   ],
   "source": [
    "# sent_tokenize\n",
    "st = sent_tokenize(raw_text_01)\n",
    "print(st, len(st))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad9452d-f284-4274-b47c-9412b1ff080e",
   "metadata": {},
   "source": [
    "### 여러 문장에 토큰 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "5e480527-1e57-42ee-ae86-6be147e6f97e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sent => ['Caution: when tokenizing a Unicode string, make sure you are not using an encoded version of the string (it may be necessary to decode it first, e.g.', 'with s.decode(\"utf8\").']\n",
      "ele => C\n",
      "wordResult => ['C']\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'set' object has no attribute 'append'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [89]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m         wordResult\u001b[38;5;241m=\u001b[39mword_tokenize(ele)\n\u001b[0;32m     14\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwordResult => \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwordResult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 15\u001b[0m         \u001b[43mtotal_token\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappend\u001b[49m(wordResult)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtotal_token => \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_token\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'set' object has no attribute 'append'"
     ]
    }
   ],
   "source": [
    "# 문장 단위로 추출\n",
    "for sent in raw_text_list:\n",
    "    total_token=set() # 중복 안하려면\n",
    "    \n",
    "    # 문장 추출\n",
    "    sentResult=sent_tokenize(sent)\n",
    "    \n",
    "    # 문장에서 추출한 토큰\n",
    "    print(f'sent => {sentResult}')\n",
    "        \n",
    "    for ele in sent:\n",
    "        print(f'ele => {ele}')\n",
    "        wordResult=word_tokenize(ele)\n",
    "        print(f'wordResult => {wordResult}')\n",
    "        total_token.append(wordResult)\n",
    "\n",
    "print(f'total_token => {total_token}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061adcc7-ba62-4e7c-a654-937e71cb53fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
