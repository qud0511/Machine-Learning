{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e758e10c-fd1e-4a07-b667-0057c7dc630f",
   "metadata": {},
   "source": [
    "# 텍스트 전처리\n",
    "- 패키지 설치\n",
    "    - NLTK : pip install nltk\n",
    "    - KoNLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "23e73455-17b2-4921-878f-c9daa469c8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install --user --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "55637c55-d927-4f16-b9c8-d3adc03c45f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "5ef4af37-d1f2-4c08-a21b-dedde76b26d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install konlpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "8a2e2fc0-6ac5-4d93-8941-d141e2581dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import konlpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "6181b8d2-184c-4c90-89d7-b67e3b27dd61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk LookupError\n",
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3fe31d-0bb7-41b6-9688-b3161c55d48d",
   "metadata": {},
   "source": [
    "##  [1] 토큰화(Tokenize)\n",
    "- 문장/ 문서를 의미를 지닌 작은 단위로 나뉘는 것\n",
    "- 나누어진 단어를 토큰(Token)이라 함.\n",
    "- 종류\n",
    "    - 문장 토큰화\n",
    "    - 단어 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "aa93c647-2029-424d-8102-4b6e57cb9bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "a3c361a0-fe6f-4fce-9656-27be55207e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text_01='Caution: when tokenizing a Unicode string,\\\n",
    "make sure you are not using an encoded version of the string (it may be necessary to decode it first,\\\n",
    "e.g. with s.decode(\"utf8\").'\n",
    "\n",
    "raw_text_02=\"\"\"Return a tokenized copy of text, using NLTK’s recommended word tokenizer (currently an improved TreebankWordTokenizer along with PunktSentenceTokenizer for the specified language).\n",
    "\"\"\"\n",
    "\n",
    "raw_text_03=\"This particular tokenizer requires the Punkt sentence tokenization models to be installed.\\\n",
    "NLTK also provides a simpler, regular-expression based tokenizer, which splits text on whitespace and punctuation:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "bf65f988-c0a9-4f87-b92c-b2a7b3895b1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Caution', ':', 'when', 'tokenizing', 'a', 'Unicode', 'string', ',', 'make', 'sure', 'you', 'are', 'not', 'using', 'an', 'encoded', 'version', 'of', 'the', 'string', '(', 'it', 'may', 'be', 'necessary', 'to', 'decode', 'it', 'first', ',', 'e.g', '.', 'with', 's.decode', '(', '``', 'utf8', \"''\", ')', '.']\n"
     ]
    }
   ],
   "source": [
    "# 단어 단위 토큰화 word_tokenize\n",
    "result_01=word_tokenize(raw_text_01)\n",
    "print(result_01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "720cb553-dc26-4f18-895d-176c0f6562cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Caution: when tokenizing a Unicode string, make sure you are not using an encoded version of the string (it may be necessary to decode it first, e.g. with s.decode(\"utf8\").', 'Return a tokenized copy of text, using NLTK’s recommended word tokenizer (currently an improved TreebankWordTokenizer along with PunktSentenceTokenizer for the specified language).\\n', 'This particular tokenizer requires the Punkt sentence tokenization models to be installed.NLTK also provides a simpler, regular-expression based tokenizer, which splits text on whitespace and punctuation:']\n"
     ]
    }
   ],
   "source": [
    "raw_text_list=[raw_text01, raw_text_02, raw_text_03]\n",
    "print(raw_text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "67ce8232-27ee-4181-a7d5-26c54dc699fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Caution: when tokenizing a Unicode string,make sure you are not using an encoded version of the string (it may be necessary to decode it first,e.g.', 'with s.decode(\"utf8\").'] 2\n"
     ]
    }
   ],
   "source": [
    "# sent_tokenize\n",
    "st = sent_tokenize(raw_text_01)\n",
    "print(st, len(st))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad9452d-f284-4274-b47c-9412b1ff080e",
   "metadata": {},
   "source": [
    "### 여러 문장에 토큰 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "5e480527-1e57-42ee-ae86-6be147e6f97e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # 문장 단위로 추출\n",
    "# for sent in raw_text_list:\n",
    "#     total_token=[] # 중복 안하려면 set()\n",
    "    \n",
    "#     # 문장 추출\n",
    "#     sentResult=sent_tokenize(sent)\n",
    "    \n",
    "#     # 문장에서 추출한 토큰\n",
    "#     print(f'sent => {sentResult}')\n",
    "        \n",
    "#     for ele in sent:\n",
    "#         print(f'ele => {ele}')\n",
    "#         wordResult=word_tokenize(ele)\n",
    "#         print(f'wordResult => {wordResult}')\n",
    "#         total_token.append(wordResult)\n",
    "\n",
    "# print(f'total_token => {total_token}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87cebc8-9a6b-40d0-a2ae-6cf19a1c64bc",
   "metadata": {
    "tags": []
   },
   "source": [
    "## [2] 정제 & 정규화\n",
    "- 불용어 제거 => 노이즈 제거\n",
    "- 텍스트의 동일화\n",
    "    - 대문자 또는 소문자로 통일\n",
    "    - 문장의 길이"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "e735ef79-19d1-4a65-8fd3-1ed8c3b69e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "## [2-1] 불용어 (Stopword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "cca78f33-83c6-4703-ae28-a334be7de74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.corpus.stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "77f45c19-f1c2-4563-819a-6643f4859711",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_stopwords=nltk.corpus.stopwords.words('english')\n",
    "len(en_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "efbbec7c-8da5-441a-8983-ac25cb8aeade",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_stopwords[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f764858e-2302-4da3-846e-f737fa681fc3",
   "metadata": {},
   "source": [
    "## [2-2] 어간 및 표제어 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "9f096551-ecc9-4291-8580-10bb383506d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import LancasterStemmer # 어간만 자름"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "e1112d0b-f7f5-4d18-8561-6d86b27d1a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstem=LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "c68afc67-30cc-4fce-9acf-3996e77faf52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('happy', 'happy')"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstem.stem('happy'), lstem.stem('happiness')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "784c2a16-9af9-407d-8379-b1c576e0fd5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('amus', 'amus')"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstem.stem('amuse'), lstem.stem('amused') # amuse 원래 단어의 일부가 잘리는 현상 발생"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "85d08d33-9714-4992-8ad1-2982f3c46ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 표제어(사전에 등록된 단어 추출)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "0ef6ba96-33e0-4405-81f6-b6e1b64f5be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "6ca0e628-11f4-45aa-9493-0416d3f0f4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "wlema=WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "4e186fc8-6db2-4afb-8704-7691ce9d2f6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('work', 'work')"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wlema.lemmatize('working', 'v'), wlema.lemmatize('worked', 'v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "1587b1cc-f046-49e1-ad24-d9996a2ef620",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('amuse', 'amuse')"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wlema.lemmatize('amusing', 'v'), wlema.lemmatize('amused', 'v')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894223c2-0c29-46d5-9803-102651084c6f",
   "metadata": {},
   "source": [
    "## [3] 텍스트 벡터화\n",
    "- 텍스트를 수치화\n",
    "- 희소벡터(OHE) : BOW 방식 --> Count 기반, TF-IDF 기반\n",
    "- 밀집벡터(공간행렬) : Embedding 방식, Word2Vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "ada9561b-e030-4af2-a5d0-0f518e323b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "d3c9e47d-1a17-4f4e-bb7a-96f1e74de995",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus=[raw_text_01, raw_text_02]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "15e4ea7f-4197-4de8-8aca-fadd9696b2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe=CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "2322d243-2855-46c1-bf2a-55a26de3193e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 4)\t1\n",
      "  (0, 37)\t1\n",
      "  (0, 31)\t1\n",
      "  (0, 33)\t1\n",
      "  (0, 24)\t2\n",
      "  (0, 14)\t1\n",
      "  (0, 25)\t1\n",
      "  (0, 40)\t1\n",
      "  (0, 2)\t1\n",
      "  (0, 18)\t1\n",
      "  (0, 34)\t1\n",
      "  (0, 1)\t1\n",
      "  (0, 8)\t1\n",
      "  (0, 36)\t1\n",
      "  (0, 19)\t1\n",
      "  (0, 27)\t1\n",
      "  (0, 12)\t2\n",
      "  (0, 15)\t1\n",
      "  (0, 3)\t1\n",
      "  (0, 16)\t1\n",
      "  (0, 28)\t1\n",
      "  (0, 7)\t2\n",
      "  (0, 9)\t1\n",
      "  (0, 38)\t1\n",
      "  (0, 35)\t1\n",
      "  (1, 34)\t1\n",
      "  (1, 1)\t1\n",
      "  (1, 19)\t1\n",
      "  (1, 27)\t1\n",
      "  (1, 38)\t1\n",
      "  (1, 22)\t1\n",
      "  (1, 29)\t1\n",
      "  (1, 5)\t1\n",
      "  (1, 26)\t1\n",
      "  (1, 17)\t1\n",
      "  (1, 21)\t1\n",
      "  (1, 39)\t1\n",
      "  (1, 30)\t1\n",
      "  (1, 6)\t1\n",
      "  (1, 11)\t1\n",
      "  (1, 32)\t1\n",
      "  (1, 0)\t1\n",
      "  (1, 20)\t1\n",
      "  (1, 10)\t1\n",
      "  (1, 23)\t1\n",
      "  (1, 13)\t1\n"
     ]
    }
   ],
   "source": [
    "result=ohe.fit_transform(corpus)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "73abbfac-b476-498c-823c-7940ec641de4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 41) [[0 1 1 1 1 0 0 2 1 1 0 0 2 0 1 1 1 0 1 1 0 0 0 0 2 1 0 1 1 0 0 1 0 1 1 1\n",
      "  1 1 1 0 1]\n",
      " [1 1 0 0 0 1 1 0 0 0 1 1 0 1 0 0 0 1 0 1 1 1 1 1 0 0 1 1 0 1 1 0 1 0 1 0\n",
      "  0 0 1 1 0]]\n"
     ]
    }
   ],
   "source": [
    "result=result.toarray()\n",
    "print(result.shape, result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "60d661da-ce43-486a-9f2d-77b15c876f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## DF-IDF 기반\n",
    "tfldf=TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "7b9e521e-a361-4e87-85a2-f61217fd7842",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_corpus=tfldf.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "f3ce6cc4-74c7-4da0-8dc8-76845893d2df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse.csr.csr_matrix"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tf_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "e719780c-6370-4143-a87f-ddf1c514d92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_corpus=tf_corpus.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "1b8f4a1a-7ee3-42d7-87e0-7b2560294937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.12670961 0.17808593 0.17808593 0.17808593 0.\n",
      "  0.         0.35617186 0.17808593 0.17808593 0.         0.\n",
      "  0.35617186 0.         0.17808593 0.17808593 0.17808593 0.\n",
      "  0.17808593 0.12670961 0.         0.         0.         0.\n",
      "  0.35617186 0.17808593 0.         0.12670961 0.17808593 0.\n",
      "  0.         0.17808593 0.         0.17808593 0.12670961 0.17808593\n",
      "  0.17808593 0.17808593 0.12670961 0.         0.17808593]\n",
      " [0.23229935 0.1652829  0.         0.         0.         0.23229935\n",
      "  0.23229935 0.         0.         0.         0.23229935 0.23229935\n",
      "  0.         0.23229935 0.         0.         0.         0.23229935\n",
      "  0.         0.1652829  0.23229935 0.23229935 0.23229935 0.23229935\n",
      "  0.         0.         0.23229935 0.1652829  0.         0.23229935\n",
      "  0.23229935 0.         0.23229935 0.         0.1652829  0.\n",
      "  0.         0.         0.1652829  0.23229935 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(tf_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "c20536ee-112b-4896-8ad2-16312866e16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install -U spacy\n",
    "# https://spacy.io/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "0a548126-022d-4b33-b1d9-eb897401a43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 토크나이저 객체 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "be17de08-1289-4967-83ff-bf36cebf9bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence, Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "88d593bd-a57f-49c7-b5ef-1f10edf1ad1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Caution: when tokenizing a Unicode string, make sure you are not using an encoded version of the string (it may be necessary to decode it first, e.g. with s.decode(\"utf8\").',\n",
       "       'Return a tokenized copy of text, using NLTK’s recommended word tokenizer (currently an improved TreebankWordTokenizer along with PunktSentenceTokenizer for the specified language).\\n'],\n",
       "      dtype='<U181')"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "88cafd4f-348b-468d-821b-cad7c2087279",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [166]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 토큰으로 나누기\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m tokens\u001b[38;5;241m=\u001b[39m\u001b[43mtext_to_word_sequence\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_text\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\preprocessing\\text.py:74\u001b[0m, in \u001b[0;36mtext_to_word_sequence\u001b[1;34m(input_text, filters, lower, split)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Converts a text to a sequence of words (or tokens).\u001b[39;00m\n\u001b[0;32m     47\u001b[0m \n\u001b[0;32m     48\u001b[0m \u001b[38;5;124;03mDeprecated: `tf.keras.preprocessing.text.text_to_word_sequence` does not\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;124;03m    A list of words (or tokens).\u001b[39;00m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lower:\n\u001b[1;32m---> 74\u001b[0m     input_text \u001b[38;5;241m=\u001b[39m \u001b[43minput_text\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlower\u001b[49m()\n\u001b[0;32m     76\u001b[0m translate_dict \u001b[38;5;241m=\u001b[39m {c: split \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m filters}\n\u001b[0;32m     77\u001b[0m translate_map \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m\u001b[38;5;241m.\u001b[39mmaketrans(translate_dict)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "# 토큰으로 나누기\n",
    "tokens=text_to_word_sequence(raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c7b944-a8f0-49b1-b776-5b026453841d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(tokens, tes))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
