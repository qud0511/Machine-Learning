{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 텍스트 전처리\n",
    "- https://www.nltk.org/\n",
    "- 패키지 설치\n",
    "  - NLTK : pip intstall nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting nltk\n",
      "  Downloading nltk-3.7-py3-none-any.whl (1.5 MB)\n",
      "     ---------------------------------------- 1.5/1.5 MB 1.6 MB/s eta 0:00:00\n",
      "Collecting click\n",
      "  Downloading click-8.1.3-py3-none-any.whl (96 kB)\n",
      "     ---------------------------------------- 96.6/96.6 kB 1.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: tqdm in c:\\users\\whrjs\\appdata\\roaming\\python\\python39\\site-packages (from nltk) (4.64.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\whrjs\\appdata\\roaming\\python\\python39\\site-packages (from nltk) (1.1.0)\n",
      "Collecting regex>=2021.8.3\n",
      "  Downloading regex-2022.9.13-cp39-cp39-win_amd64.whl (267 kB)\n",
      "     -------------------------------------- 267.7/267.7 kB 2.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\envs\\ev_py39\\lib\\site-packages (from click->nltk) (0.4.5)\n",
      "Installing collected packages: regex, click, nltk\n",
      "Successfully installed click-8.1.3 nltk-3.7 regex-2022.9.13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script nltk.exe is installed in 'C:\\Users\\whrjs\\AppData\\Roaming\\Python\\Python39\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"
     ]
    }
   ],
   "source": [
    "# NLTK 패키지 설치\n",
    "# !pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: konlpy in c:\\users\\whrjs\\appdata\\roaming\\python\\python39\\site-packages (0.6.0)\n",
      "Requirement already satisfied: numpy>=1.6 in c:\\users\\whrjs\\appdata\\roaming\\python\\python39\\site-packages (from konlpy) (1.22.4)\n",
      "Requirement already satisfied: JPype1>=0.7.0 in c:\\users\\whrjs\\appdata\\roaming\\python\\python39\\site-packages (from konlpy) (1.4.0)\n",
      "Requirement already satisfied: lxml>=4.1.0 in c:\\users\\whrjs\\appdata\\roaming\\python\\python39\\site-packages (from konlpy) (4.9.1)\n"
     ]
    }
   ],
   "source": [
    "# konlpy 패키지 설치\n",
    "# !pip install konlpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [1] 토큰화(Tokenization)\n",
    "- 문장/문서 의미를 지닌 작은 단위로 나누는 것\n",
    "- 나누어진 단어를 토큰(Token)이라고 함\n",
    "- 종류\n",
    "  - 문장 토큰화\n",
    "  - 단어 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize,word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NLTK Corpus 말뭉치 데이터셋 다운로드 받기\n",
    "nltk.download('all',quiet=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- quiet=True옵션은 다운로드 받는 화면이 나오지 않음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text1=\"\"\"\n",
    "NLTK Tokenizer Package Tokenizers divide strings into lists of substrings.\n",
    "For example, tokenizers can be used to find the words and punctuation in a string:\n",
    "\"\"\"\n",
    "\n",
    "raw_text2=\"\"\"\n",
    "This particular tokenizer requires the Punkt sentence tokenization models to be installed. NLTK also provides a simpler, regular-expression based tokenizer, which splits text on whitespace and punctuation:\n",
    "\"\"\"\n",
    "\n",
    "raw_text3=\"\"\"\n",
    "Caution: when tokenizing a Unicode string, make sure you are not using an encoded version of the string (it may be necessary to decode it first, e.g. with s.decode(\"utf8\").\n",
    "\n",
    "NLTK tokenizers can produce token-spans, represented as tuples of integers having the same semantics as string slices, to support efficient comparison of tokenizers. (These methods are implemented as generators.)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어 단위 토큰화\n",
    "result1=word_tokenize(raw_text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NLTK', 'Tokenizer', 'Package', 'Tokenizers', 'divide', 'strings', 'into', 'lists', 'of', 'substrings', '.', 'For', 'example', ',', 'tokenizers', 'can', 'be', 'used', 'to', 'find', 'the', 'words', 'and', 'punctuation', 'in', 'a', 'string', ':']\n"
     ]
    }
   ],
   "source": [
    "print(result1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'particular', 'tokenizer', 'requires', 'the', 'Punkt', 'sentence', 'tokenization', 'models', 'to', 'be', 'installed', '.', 'NLTK', 'also', 'provides', 'a', 'simpler', ',', 'regular-expression', 'based', 'tokenizer', ',', 'which', 'splits', 'text', 'on', 'whitespace', 'and', 'punctuation', ':']\n"
     ]
    }
   ],
   "source": [
    "result2=word_tokenize(raw_text2)\n",
    "print(result2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result2=word_tokenize(raw_text2)\n",
    "print(result2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여러개 문장의 변수를 생성\n",
    "raw_text4=\"when tokenizing a Unicode string.\\\n",
    "           make sure you are not using an encoded version of the string.\\\n",
    "           NLTK tokenizers can produce token-spans, represented as tuples of integers having the same semantics as string slices, to support efficient comparison of tokenizers.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장 단위 토큰화\n",
    "sent_result=sent_tokenize(raw_text4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['when tokenizing a Unicode string.', 'make sure you are not using an encoded version of the string.', 'NLTK tokenizers can produce token-spans, represented as tuples of integers having the same semantics as string slices, to support efficient comparison of tokenizers.'] 3\n"
     ]
    }
   ],
   "source": [
    "print(sent_result,len(sent_result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 여러 문장에 토큰 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=['when tokenizing a Unicode string.',\n",
    "           'make sure you are not using an encoded version of the string.',\n",
    "           'NLTK tokenizers can produce token-spans, represented as tuples of integers having the same semantics as string slices, to support efficient comparison of tokenizers.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sent => when tokenizing a Unicode string.\n",
      "sentToken => ['when', 'tokenizing', 'a', 'Unicode', 'string', '.']\n",
      "sent => make sure you are not using an encoded version of the string.\n",
      "sentToken => ['make', 'sure', 'you', 'are', 'not', 'using', 'an', 'encoded', 'version', 'of', 'the', 'string', '.']\n",
      "sent => NLTK tokenizers can produce token-spans, represented as tuples of integers having the same semantics as string slices, to support efficient comparison of tokenizers.\n",
      "sentToken => ['NLTK', 'tokenizers', 'can', 'produce', 'token-spans', ',', 'represented', 'as', 'tuples', 'of', 'integers', 'having', 'the', 'same', 'semantics', 'as', 'string', 'slices', ',', 'to', 'support', 'efficient', 'comparison', 'of', 'tokenizers', '.']\n",
      "[['when', 'tokenizing', 'a', 'Unicode', 'string', '.'], ['make', 'sure', 'you', 'are', 'not', 'using', 'an', 'encoded', 'version', 'of', 'the', 'string', '.'], ['NLTK', 'tokenizers', 'can', 'produce', 'token-spans', ',', 'represented', 'as', 'tuples', 'of', 'integers', 'having', 'the', 'same', 'semantics', 'as', 'string', 'slices', ',', 'to', 'support', 'efficient', 'comparison', 'of', 'tokenizers', '.']] 3\n"
     ]
    }
   ],
   "source": [
    "# 문장 단위로 추출\n",
    "total_token=[]\n",
    "for sent in text:\n",
    "    \n",
    "    # 문장에서 추출한 토큰\n",
    "    print(f\"sent => {sent}\")\n",
    "    sentToken=word_tokenize(sent)\n",
    "    print(f\"sentToken => {sentToken}\")\n",
    "\n",
    "    # 모든 문장의 토큰에 추가\n",
    "    total_token.append(sentToken)\n",
    "print(total_token,len(total_token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sent => when tokenizing a Unicode string.\n",
      "ele => when tokenizing a Unicode string.\n",
      "wordResult => ['when', 'tokenizing', 'a', 'Unicode', 'string', '.']\n",
      "sent => make sure you are not using an encoded version of the string.\n",
      "ele => make sure you are not using an encoded version of the string.\n",
      "wordResult => ['make', 'sure', 'you', 'are', 'not', 'using', 'an', 'encoded', 'version', 'of', 'the', 'string', '.']\n",
      "sent => NLTK tokenizers can produce token-spans, represented as tuples of integers having the same semantics as string slices, to support efficient comparison of tokenizers.\n",
      "ele => NLTK tokenizers can produce token-spans, represented as tuples of integers having the same semantics as string slices, to support efficient comparison of tokenizers.\n",
      "wordResult => ['NLTK', 'tokenizers', 'can', 'produce', 'token-spans', ',', 'represented', 'as', 'tuples', 'of', 'integers', 'having', 'the', 'same', 'semantics', 'as', 'string', 'slices', ',', 'to', 'support', 'efficient', 'comparison', 'of', 'tokenizers', '.']\n",
      "[['when', 'tokenizing', 'a', 'Unicode', 'string', '.'], ['make', 'sure', 'you', 'are', 'not', 'using', 'an', 'encoded', 'version', 'of', 'the', 'string', '.'], ['NLTK', 'tokenizers', 'can', 'produce', 'token-spans', ',', 'represented', 'as', 'tuples', 'of', 'integers', 'having', 'the', 'same', 'semantics', 'as', 'string', 'slices', ',', 'to', 'support', 'efficient', 'comparison', 'of', 'tokenizers', '.']]\n"
     ]
    }
   ],
   "source": [
    "# 문장 단위로 추출\n",
    "total_token=[]\n",
    "for sent in text:\n",
    "    # 문장 추출\n",
    "    sentResult=sent_tokenize(sent)\n",
    "\n",
    "    print(f\"sent => {sent}\")\n",
    "\n",
    "    for ele in  sentResult:\n",
    "        print(f\"ele => {ele}\")\n",
    "        wordResult=word_tokenize(ele)\n",
    "        print(f\"wordResult => {wordResult}\")\n",
    "        total_token.append(wordResult)\n",
    "        \n",
    "print(total_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 한글"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 형태소 분리 객체\n",
    "okt=Okt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 형태소 분히 휴 태깅(Tagging) => 품사\n",
    "result2=okt.pos('오늘은 월요일입니다.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('오늘', 'Noun'), ('은', 'Josa'), ('월요일', 'Noun'), ('입니다', 'Adjective'), ('.', 'Punctuation')]\n"
     ]
    }
   ],
   "source": [
    "print(result2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "result2=okt.pos('오늘은 월요일입니다.',stem=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('오늘', 'Noun'), ('은', 'Josa'), ('월요일', 'Noun'), ('이다', 'Adjective'), ('.', 'Punctuation')]\n"
     ]
    }
   ],
   "source": [
    "print(result2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- '입니다'가 아닌 '이다'가 나옴"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [2] 정제 & 정규화\n",
    "- 불용어 제거 => 노이즈 제거\n",
    "- 텍스트의 동일화\n",
    "  - 대문자 또는 소문자로 동일\n",
    "  - 문장의 길이"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [2-1] 불용어(Stopword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_stopwords=nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(en_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_stopwords[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [2-2] 어간(Stemming) 및 표제어 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import LancasterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 객체생성 => 어간 추출\n",
    "stemmer=LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "work work work\n",
      "happy happy\n",
      "amus amus\n"
     ]
    }
   ],
   "source": [
    "print(stemmer.stem('working'),stemmer.stem('worked'),stemmer.stem('worken'))\n",
    "print(stemmer.stem('happy'),stemmer.stem('happiness'))\n",
    "print(stemmer.stem('amuse'),stemmer.stem('amused'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 객체생성 => 표제어(사전에 등록된 단어 추출)\n",
    "wordnet=WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "work work\n",
      "amuse amuse\n"
     ]
    }
   ],
   "source": [
    "print(wordnet.lemmatize('working','v'),wordnet.lemmatize('worked','v'))\n",
    "print(wordnet.lemmatize('amusing','v'),wordnet.lemmatize('amused','v'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [3] 텍스트 벡터화\n",
    "- 텍스트 => 수치화\n",
    "- 희소 벡터(OHE) : BOW 방식 --> Count기반, TF-IDF 기반\n",
    "- 밀집 벡터 : Embedding 방식, Word2Vect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [3-1] OHE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_test='This is a pen.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus=[raw_text1,raw_text2,ex_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe=CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret=ohe.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse._csr.csr_matrix'>\n",
      "  (0, 16)\t1\n",
      "  (0, 38)\t1\n",
      "  (0, 19)\t1\n",
      "  (0, 39)\t2\n",
      "  (0, 5)\t1\n",
      "  (0, 31)\t1\n",
      "  (0, 12)\t1\n",
      "  (0, 14)\t1\n",
      "  (0, 17)\t1\n",
      "  (0, 32)\t1\n",
      "  (0, 9)\t1\n",
      "  (0, 6)\t1\n",
      "  (0, 4)\t1\n",
      "  (0, 3)\t1\n",
      "  (0, 40)\t1\n",
      "  (0, 36)\t1\n",
      "  (0, 8)\t1\n",
      "  (0, 34)\t1\n",
      "  (0, 43)\t1\n",
      "  (0, 1)\t1\n",
      "  (0, 23)\t1\n",
      "  (0, 10)\t1\n",
      "  (0, 30)\t1\n",
      "  (1, 16)\t1\n",
      "  (1, 38)\t2\n",
      "  :\t:\n",
      "  (1, 34)\t1\n",
      "  (1, 1)\t1\n",
      "  (1, 23)\t1\n",
      "  (1, 35)\t1\n",
      "  (1, 20)\t1\n",
      "  (1, 26)\t1\n",
      "  (1, 24)\t1\n",
      "  (1, 27)\t1\n",
      "  (1, 37)\t1\n",
      "  (1, 15)\t1\n",
      "  (1, 11)\t1\n",
      "  (1, 0)\t1\n",
      "  (1, 22)\t1\n",
      "  (1, 28)\t1\n",
      "  (1, 25)\t1\n",
      "  (1, 7)\t1\n",
      "  (1, 2)\t1\n",
      "  (1, 41)\t1\n",
      "  (1, 29)\t1\n",
      "  (1, 33)\t1\n",
      "  (1, 18)\t1\n",
      "  (1, 42)\t1\n",
      "  (2, 35)\t1\n",
      "  (2, 13)\t1\n",
      "  (2, 21)\t1\n"
     ]
    }
   ],
   "source": [
    "print(type(ret),ret,sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret=ret.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 44) [[0 1 0 1 1 1 1 0 1 1 1 0 1 0 1 0 1 1 0 1 0 0 0 1 0 0 0 0 0 0 1 1 1 0 1 0\n",
      "  1 0 1 2 1 0 0 1]\n",
      " [1 1 1 1 0 0 0 1 0 0 0 1 0 0 0 1 1 0 1 0 1 0 1 1 1 1 1 1 1 1 0 0 0 1 1 1\n",
      "  1 1 2 0 0 1 1 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1\n",
      "  0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(ret.shape,ret)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [3-2] TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf=TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_corpus=tfidf.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse._csr.csr_matrix"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tf_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_corpus=tf_corpus.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.15841263 0.         0.15841263 0.20829358 0.20829358\n",
      "  0.20829358 0.         0.20829358 0.20829358 0.20829358 0.\n",
      "  0.20829358 0.         0.20829358 0.         0.15841263 0.20829358\n",
      "  0.         0.20829358 0.         0.         0.         0.15841263\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.20829358 0.20829358 0.20829358 0.         0.15841263 0.\n",
      "  0.15841263 0.         0.15841263 0.41658715 0.20829358 0.\n",
      "  0.         0.20829358]\n",
      " [0.20260027 0.15408273 0.20260027 0.15408273 0.         0.\n",
      "  0.         0.20260027 0.         0.         0.         0.20260027\n",
      "  0.         0.         0.         0.20260027 0.15408273 0.\n",
      "  0.20260027 0.         0.20260027 0.         0.20260027 0.15408273\n",
      "  0.20260027 0.20260027 0.20260027 0.20260027 0.20260027 0.20260027\n",
      "  0.         0.         0.         0.20260027 0.15408273 0.15408273\n",
      "  0.15408273 0.20260027 0.30816545 0.         0.         0.20260027\n",
      "  0.20260027 0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.62276601 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.62276601 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.4736296\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.        ]] (3, 44)\n"
     ]
    }
   ],
   "source": [
    "print(tf_corpus,tf_corpus.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 실습\n",
    "- 불용어 뽑아내기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent='Wiki is in Ward is original description: The simplest online database that could possibly work.\\\n",
    "Wiki is a piece of server software that allows users to freely create and edit Web page content using any Web browser. Wiki supports hyperlinks and has a simple text syntax for creating new pages and crosslinks between internal pages on the fly.\\\n",
    "Wiki is unusual among group communication mechanisms in that it allows the organization of contributions to be edited in addition to the content itself.Like many simple concepts, \"open editing\" has some profound and subtle effects on Wiki usage. Allowing everyday users to create and edit any page in a Web site is exciting in that it encourages democratic use of the Web and promotes content composition by nontechnical users.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 영어 불용어\n",
    "en_stwo=set(nltk.corpus.stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wiki is in Ward is original description: The simplest online database that could possibly work.Wiki is a piece of server software that allows users to freely create and edit Web page content using any Web browser. Wiki supports hyperlinks and has a simple text syntax for creating new pages and crosslinks between internal pages on the fly.Wiki is unusual among group communication mechanisms in that it allows the organization of contributions to be edited in addition to the content itself.Like many simple concepts, \"open editing\" has some profound and subtle effects on Wiki usage. Allowing everyday users to create and edit any page in a Web site is exciting in that it encourages democratic use of the Web and promotes content composition by nontechnical users.문장의 불용어는??\n",
      "['is', 'in', 'is', 'that', 'is', 'a', 'of', 'that', 'to', 'and', 'any', 'and', 'has', 'a', 'for', 'and', 'between', 'on', 'the', 'is', 'in', 'that', 'it', 'the', 'of', 'to', 'be', 'in', 'to', 'the', 'has', 'some', 'and', 'on', 'to', 'and', 'any', 'in', 'a', 'is', 'in', 'that', 'it', 'of', 'the', 'and', 'by']\n"
     ]
    }
   ],
   "source": [
    "print(f\"{sent}문장의 불용어는??\")\n",
    "stopword_List=[]\n",
    "for st in word_tokenize(sent):\n",
    "    if st in en_stwo:\n",
    "        stopword_List.append(st)\n",
    "print(stopword_List)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wiki is in Ward is original description: The simplest online database that could possibly work.Wiki is a piece of server software that allows users to freely create and edit Web page content using any Web browser. Wiki supports hyperlinks and has a simple text syntax for creating new pages and crosslinks between internal pages on the fly.Wiki is unusual among group communication mechanisms in that it allows the organization of contributions to be edited in addition to the content itself.Like many simple concepts, \"open editing\" has some profound and subtle effects on Wiki usage. Allowing everyday users to create and edit any page in a Web site is exciting in that it encourages democratic use of the Web and promotes content composition by nontechnical users.문장의 불용어는??\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "85"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result=[]\n",
    "for sen in sent.split('\\\\'):\n",
    "    print(f\"{sen}문장의 불용어는??\")\n",
    "    sen_token=word_tokenize(sen)\n",
    "    for st in sen_token:\n",
    "        if st not in en_stwo:\n",
    "            result.append(st)\n",
    "len(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Wiki is in Ward is original description: The simplest online database that could possibly work.Wiki is a piece of server software that allows users to freely create and edit Web page content using any Web browser. Wiki supports hyperlinks and has a simple text syntax for creating new pages and crosslinks between internal pages on the fly.Wiki is unusual among group communication mechanisms in that it allows the organization of contributions to be edited in addition to the content itself.Like many simple concepts, \"open editing\" has some profound and subtle effects on Wiki usage. Allowing everyday users to create and edit any page in a Web site is exciting in that it encourages democratic use of the Web and promotes content composition by nontechnical users.'"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Wiki',\n",
       " 'Ward',\n",
       " 'original',\n",
       " 'description',\n",
       " ':',\n",
       " 'The',\n",
       " 'simplest',\n",
       " 'online',\n",
       " 'database',\n",
       " 'could',\n",
       " 'possibly',\n",
       " 'work.Wiki',\n",
       " 'piece',\n",
       " 'server',\n",
       " 'software',\n",
       " 'allows',\n",
       " 'users',\n",
       " 'freely',\n",
       " 'create',\n",
       " 'edit',\n",
       " 'Web',\n",
       " 'page',\n",
       " 'content',\n",
       " 'using',\n",
       " 'Web',\n",
       " 'browser',\n",
       " '.',\n",
       " 'Wiki',\n",
       " 'supports',\n",
       " 'hyperlinks',\n",
       " 'simple',\n",
       " 'text',\n",
       " 'syntax',\n",
       " 'creating',\n",
       " 'new',\n",
       " 'pages',\n",
       " 'crosslinks',\n",
       " 'internal',\n",
       " 'pages',\n",
       " 'fly.Wiki',\n",
       " 'unusual',\n",
       " 'among',\n",
       " 'group',\n",
       " 'communication',\n",
       " 'mechanisms',\n",
       " 'allows',\n",
       " 'organization',\n",
       " 'contributions',\n",
       " 'edited',\n",
       " 'addition',\n",
       " 'content',\n",
       " 'itself.Like',\n",
       " 'many',\n",
       " 'simple',\n",
       " 'concepts',\n",
       " ',',\n",
       " '``',\n",
       " 'open',\n",
       " 'editing',\n",
       " \"''\",\n",
       " 'profound',\n",
       " 'subtle',\n",
       " 'effects',\n",
       " 'Wiki',\n",
       " 'usage',\n",
       " '.',\n",
       " 'Allowing',\n",
       " 'everyday',\n",
       " 'users',\n",
       " 'create',\n",
       " 'edit',\n",
       " 'page',\n",
       " 'Web',\n",
       " 'site',\n",
       " 'exciting',\n",
       " 'encourages',\n",
       " 'democratic',\n",
       " 'use',\n",
       " 'Web',\n",
       " 'promotes',\n",
       " 'content',\n",
       " 'composition',\n",
       " 'nontechnical',\n",
       " 'users',\n",
       " '.']"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 리스트 내포\n",
    "wordTokens=word_tokenize(sent)\n",
    "[word for word in wordTokens if word not in en_stwo]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'.', ':'}"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{':','.'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wiki is in Ward is original description: The simplest online database that could possibly work.Wiki is a piece of server software that allows users to freely create and edit Web page content using any Web browser. Wiki supports hyperlinks and has a simple text syntax for creating new pages and crosslinks between internal pages on the fly.Wiki is unusual among group communication mechanisms in that it allows the organization of contributions to be edited in addition to the content itself.Like many simple concepts, \"open editing\" has some profound and subtle effects on Wiki usage. Allowing everyday users to create and edit any page in a Web site is exciting in that it encourages democratic use of the Web and promotes content composition by nontechnical users.문장의 불용어는??\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +: 'set' and 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\deeplearning\\0926\\ex_nltk.ipynb 셀 64\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/deeplearning/0926/ex_nltk.ipynb#Y136sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m sen_token\u001b[39m=\u001b[39mword_tokenize(sen)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/deeplearning/0926/ex_nltk.ipynb#Y136sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mfor\u001b[39;00m st \u001b[39min\u001b[39;00m sen_token:\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/deeplearning/0926/ex_nltk.ipynb#Y136sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39mif\u001b[39;00m st \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m en_stwo\u001b[39m+\u001b[39;49m[\u001b[39m'\u001b[39;49m\u001b[39m:\u001b[39;49m\u001b[39m'\u001b[39;49m,\u001b[39m'\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m'\u001b[39;49m]:\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/deeplearning/0926/ex_nltk.ipynb#Y136sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m         result\u001b[39m.\u001b[39mappend(st)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/deeplearning/0926/ex_nltk.ipynb#Y136sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mlen\u001b[39m(result)\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for +: 'set' and 'list'"
     ]
    }
   ],
   "source": [
    "# 특수기호도 불용어로 처리하여 없애기\n",
    "result=[]\n",
    "print(f\"{sen}문장의 불용어는??\")\n",
    "sen_token=word_tokenize(sen)\n",
    "for st in sen_token:\n",
    "    if st not in en_stwo+[':','.']:\n",
    "        result.append(st)\n",
    "len(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer 객체 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "from keras.preprocessing.text import text_to_word_sequence,Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text='Wiki is in Ward is original description: The simplest online database that could possibly work.\\\n",
    "Wiki is a piece of server software that allows users to freely create and edit Web page content using any Web browser. Wiki supports hyperlinks and has a simple text syntax for creating new pages and crosslinks between internal pages on the fly.\\\n",
    "Wiki is unusual among group communication mechanisms in that it allows the organization of contributions to be edited in addition to the content itself.Like many simple concepts, \"open editing\" has some profound and subtle effects on Wiki usage. Allowing everyday users to create and edit any page in a Web site is exciting in that it encourages democratic use of the Web and promotes content composition by nontechnical users.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토큰으로 나누기\n",
    "tokens=text_to_word_sequence(raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128 ['wiki', 'is', 'in', 'ward', 'is', 'original', 'description', 'the', 'simplest', 'online', 'database', 'that', 'could', 'possibly', 'work', 'wiki', 'is', 'a', 'piece', 'of', 'server', 'software', 'that', 'allows', 'users', 'to', 'freely', 'create', 'and', 'edit', 'web', 'page', 'content', 'using', 'any', 'web', 'browser', 'wiki', 'supports', 'hyperlinks', 'and', 'has', 'a', 'simple', 'text', 'syntax', 'for', 'creating', 'new', 'pages', 'and', 'crosslinks', 'between', 'internal', 'pages', 'on', 'the', 'fly', 'wiki', 'is', 'unusual', 'among', 'group', 'communication', 'mechanisms', 'in', 'that', 'it', 'allows', 'the', 'organization', 'of', 'contributions', 'to', 'be', 'edited', 'in', 'addition', 'to', 'the', 'content', 'itself', 'like', 'many', 'simple', 'concepts', 'open', 'editing', 'has', 'some', 'profound', 'and', 'subtle', 'effects', 'on', 'wiki', 'usage', 'allowing', 'everyday', 'users', 'to', 'create', 'and', 'edit', 'any', 'page', 'in', 'a', 'web', 'site', 'is', 'exciting', 'in', 'that', 'it', 'encourages', 'democratic', 'use', 'of', 'the', 'web', 'and', 'promotes', 'content', 'composition', 'by', 'nontechnical', 'users']\n"
     ]
    }
   ],
   "source": [
    "print(len(tokens),tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토큰 객체 생성\n",
    "myToken=Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "myToken.fit_on_texts(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'and': 1, 'wiki': 2, 'is': 3, 'in': 4, 'the': 5, 'that': 6, 'to': 7, 'web': 8, 'a': 9, 'of': 10, 'users': 11, 'content': 12, 'allows': 13, 'create': 14, 'edit': 15, 'page': 16, 'any': 17, 'has': 18, 'simple': 19, 'pages': 20, 'on': 21, 'it': 22, 'ward': 23, 'original': 24, 'description': 25, 'simplest': 26, 'online': 27, 'database': 28, 'could': 29, 'possibly': 30, 'work': 31, 'piece': 32, 'server': 33, 'software': 34, 'freely': 35, 'using': 36, 'browser': 37, 'supports': 38, 'hyperlinks': 39, 'text': 40, 'syntax': 41, 'for': 42, 'creating': 43, 'new': 44, 'crosslinks': 45, 'between': 46, 'internal': 47, 'fly': 48, 'unusual': 49, 'among': 50, 'group': 51, 'communication': 52, 'mechanisms': 53, 'organization': 54, 'contributions': 55, 'be': 56, 'edited': 57, 'addition': 58, 'itself': 59, 'like': 60, 'many': 61, 'concepts': 62, 'open': 63, 'editing': 64, 'some': 65, 'profound': 66, 'subtle': 67, 'effects': 68, 'usage': 69, 'allowing': 70, 'everyday': 71, 'site': 72, 'exciting': 73, 'encourages': 74, 'democratic': 75, 'use': 76, 'promotes': 77, 'composition': 78, 'by': 79, 'nontechnical': 80}\n"
     ]
    }
   ],
   "source": [
    "print(myToken.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('wiki', 5), ('is', 5), ('in', 5), ('ward', 1), ('original', 1), ('description', 1), ('the', 5), ('simplest', 1), ('online', 1), ('database', 1), ('that', 4), ('could', 1), ('possibly', 1), ('work', 1), ('a', 3), ('piece', 1), ('of', 3), ('server', 1), ('software', 1), ('allows', 2), ('users', 3), ('to', 4), ('freely', 1), ('create', 2), ('and', 6), ('edit', 2), ('web', 4), ('page', 2), ('content', 3), ('using', 1), ('any', 2), ('browser', 1), ('supports', 1), ('hyperlinks', 1), ('has', 2), ('simple', 2), ('text', 1), ('syntax', 1), ('for', 1), ('creating', 1), ('new', 1), ('pages', 2), ('crosslinks', 1), ('between', 1), ('internal', 1), ('on', 2), ('fly', 1), ('unusual', 1), ('among', 1), ('group', 1), ('communication', 1), ('mechanisms', 1), ('it', 2), ('organization', 1), ('contributions', 1), ('be', 1), ('edited', 1), ('addition', 1), ('itself', 1), ('like', 1), ('many', 1), ('concepts', 1), ('open', 1), ('editing', 1), ('some', 1), ('profound', 1), ('subtle', 1), ('effects', 1), ('usage', 1), ('allowing', 1), ('everyday', 1), ('site', 1), ('exciting', 1), ('encourages', 1), ('democratic', 1), ('use', 1), ('promotes', 1), ('composition', 1), ('by', 1), ('nontechnical', 1)])\n"
     ]
    }
   ],
   "source": [
    "print(myToken.word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9], [], []]\n"
     ]
    }
   ],
   "source": [
    "print(myToken.texts_to_sequences('and'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9]]\n"
     ]
    }
   ],
   "source": [
    "print(myToken.texts_to_sequences('a'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[], []]\n"
     ]
    }
   ],
   "source": [
    "print(myToken.texts_to_sequences('is'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[], [], [], []]\n"
     ]
    }
   ],
   "source": [
    "print(myToken.texts_to_sequences('wiki'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -----------------------------------------------\n",
    "- 제공한 문서/문장에 대한 단어사전(voca)\n",
    "- 단어사전(voca)에 존재하지 않는 단어 => Out of Voca : oov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "  'I love my dog',\n",
    "  'I love my cat',\n",
    "  'You love my dog!',\n",
    "  'Do you think my dog is amazing?'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer=Tokenizer(num_words=3)\n",
    "tokenizer=Tokenizer(oov_token=1)\n",
    "# tokenizer=Tokenizer(num_words=3, oov_token=1)\n",
    "# tokenizer=Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어 빈도수가 높은 순으로 낮은 정수 인덱스 부여\n",
    "tokenizer.fit_on_texts(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 1, 'my': 2, 'love': 3, 'dog': 4, 'i': 5, 'you': 6, 'cat': 7, 'do': 8, 'think': 9, 'is': 10, 'amazing': 11}\n"
     ]
    }
   ],
   "source": [
    "# 단어 : 단어인덱스\n",
    "print(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 단어의 수가 많을수록 인덱스 번호가 낮다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('i', 2), ('love', 3), ('my', 4), ('dog', 3), ('cat', 1), ('you', 2), ('do', 1), ('think', 1), ('is', 1), ('amazing', 1)])\n"
     ]
    }
   ],
   "source": [
    "# 단어 출력 개수\n",
    "print(tokenizer.word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5, 3, 2, 4], [5, 3, 2, 7], [6, 3, 2, 4], [8, 6, 9, 2, 4, 10, 11]]\n"
     ]
    }
   ],
   "source": [
    "# 문장을 생성된 사전(voca)를 기반으로 수치화\n",
    "print(tokenizer.texts_to_sequences(sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 패딩(Padding)\n",
    "- 길이가 모두 다른 문장들을 동일 길이로 맞추기 위한 과정\n",
    "- 길이 기준 설정\n",
    "- 긴 경우 => 앞/뒤 중 선택\n",
    "- 짧은 경우 => 앞 뒤 중 선택\n",
    "- 값 => 패딩에 들어갈 값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  0,  5,  3,  2,  4],\n",
       "       [ 0,  0,  0,  5,  3,  2,  7],\n",
       "       [ 0,  0,  0,  6,  3,  2,  4],\n",
       "       [ 8,  6,  9,  2,  4, 10, 11]])"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result=pad_sequences(seq_voca)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-Hot-Encoding 변환\n",
    "- sklearn OneHotEncoder 객체 생성\n",
    "- keras함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seq_voca: 4\n",
      "[[5, 3, 2, 4], [5, 3, 2, 7], [6, 3, 2, 4], [8, 6, 9, 2, 4, 10, 11]]\n"
     ]
    }
   ],
   "source": [
    "# 문장을 생성된 사전를 기반으로 수치화\n",
    "seq_voca=tokenizer.texts_to_sequences(sentences)\n",
    "print(f\"seq_voca: {len(seq_voca)}\")\n",
    "print(seq_voca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0.]], dtype=float32)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "to_categorical(seq_voca[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1.]], dtype=float32)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "to_categorical(seq_voca[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0.]], dtype=float32)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "to_categorical(seq_voca[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]], dtype=float32)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "to_categorical(seq_voca[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 1., 1., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1.]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer.texts_to_matrix(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -----------example.txt----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./example.txt') as f:\n",
    "    filedata=f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The main Henry Ford Museum building houses som...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Henry Ford Academy is the first charter school...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Freshman meet inside the main museum building ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Henry Ford Learning Institute is using the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The building received the international annual...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>See also[edit]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0\n",
       "0  The main Henry Ford Museum building houses som...\n",
       "1  Henry Ford Academy is the first charter school...\n",
       "2  Freshman meet inside the main museum building ...\n",
       "3  The Henry Ford Learning Institute is using the...\n",
       "4  The building received the international annual...\n",
       "5                                     See also[edit]"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exam_df=pd.read_table('./example.txt',header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(exam_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "example=''\n",
    "for r in range(len(exam_df)):\n",
    "    example+=str(exam_df.iloc[r,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_list=[]\n",
    "for r in range(len(exam_df)):\n",
    "    example_list.append(exam_df.iloc[r,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The main Henry Ford Museum building houses some of the classrooms for the Henry Ford AcademyHenry Ford Academy is the first charter school in the United States to be developed jointly by a global corporation, public education, and a major nonprofit cultural institution',\n",
       " ' The school is sponsored by the Ford Motor Company, Wayne County Regional Educational Service Agency and The Henry Ford Museum and admits high school students',\n",
       " ' It is located in Dearborn, Michigan on the campus of the Henry Ford museum',\n",
       " ' Enrollment is taken from a lottery in the area and totaled 467 in 2010',\n",
       " '[1]Freshman meet inside the main museum building in glass walled classrooms, while older students use a converted carousel building and Pullman cars on a siding of the Greenfield Village railroad',\n",
       " ' Classes are expected to include use of the museum artifacts, a tradition of the original Village Schools',\n",
       " ' When the Museum was established in 1929, it included a school which served grades kindergarten to college/trade school ages',\n",
       " ' The last part of the original school closed in 1969',\n",
       " 'The Henry Ford Learning Institute is using the Henry Ford Academy model for further charter schools including the Power House High in Chicago and Alameda School for Art + Design in San Antonio',\n",
       " 'The building received the international annual design award of the Council of Educational Facilities Planners International for 2001, the James D',\n",
       " ' MacConnell Award for outstanding new educational facilities',\n",
       " ' Notable attendees include Chris Stroud and Isaac Sudut',\n",
       " 'See also[edit]']"
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example.split('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The main Henry Ford Museum building houses some of the classrooms for the Henry Ford Academy',\n",
       " 'Henry Ford Academy is the first charter school in the United States to be developed jointly by a global corporation, public education, and a major nonprofit cultural institution. The school is sponsored by the Ford Motor Company, Wayne County Regional Educational Service Agency and The Henry Ford Museum and admits high school students. It is located in Dearborn, Michigan on the campus of the Henry Ford museum. Enrollment is taken from a lottery in the area and totaled 467 in 2010.[1]',\n",
       " 'Freshman meet inside the main museum building in glass walled classrooms, while older students use a converted carousel building and Pullman cars on a siding of the Greenfield Village railroad. Classes are expected to include use of the museum artifacts, a tradition of the original Village Schools. When the Museum was established in 1929, it included a school which served grades kindergarten to college/trade school ages. The last part of the original school closed in 1969.',\n",
       " 'The Henry Ford Learning Institute is using the Henry Ford Academy model for further charter schools including the Power House High in Chicago and Alameda School for Art + Design in San Antonio.',\n",
       " 'The building received the international annual design award of the Council of Educational Facilities Planners International for 2001, the James D. MacConnell Award for outstanding new educational facilities. Notable attendees include Chris Stroud and Isaac Sudut.',\n",
       " 'See also[edit]']"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [1] 문장을 리스트로 만들어 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_ex_li=Tokenizer(oov_token='oov')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_ex_li.fit_on_texts(example_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'oov': 1,\n",
       " 'the': 2,\n",
       " 'in': 3,\n",
       " 'ford': 4,\n",
       " 'of': 5,\n",
       " 'henry': 6,\n",
       " 'school': 7,\n",
       " 'a': 8,\n",
       " 'and': 9,\n",
       " 'museum': 10,\n",
       " 'for': 11,\n",
       " 'is': 12,\n",
       " 'building': 13,\n",
       " 'academy': 14,\n",
       " 'to': 15,\n",
       " 'educational': 16,\n",
       " 'main': 17,\n",
       " 'classrooms': 18,\n",
       " 'charter': 19,\n",
       " 'by': 20,\n",
       " 'high': 21,\n",
       " 'students': 22,\n",
       " 'it': 23,\n",
       " 'on': 24,\n",
       " 'use': 25,\n",
       " 'village': 26,\n",
       " 'include': 27,\n",
       " 'original': 28,\n",
       " 'schools': 29,\n",
       " 'design': 30,\n",
       " 'international': 31,\n",
       " 'award': 32,\n",
       " 'facilities': 33,\n",
       " 'houses': 34,\n",
       " 'some': 35,\n",
       " 'first': 36,\n",
       " 'united': 37,\n",
       " 'states': 38,\n",
       " 'be': 39,\n",
       " 'developed': 40,\n",
       " 'jointly': 41,\n",
       " 'global': 42,\n",
       " 'corporation': 43,\n",
       " 'public': 44,\n",
       " 'education': 45,\n",
       " 'major': 46,\n",
       " 'nonprofit': 47,\n",
       " 'cultural': 48,\n",
       " 'institution': 49,\n",
       " 'sponsored': 50,\n",
       " 'motor': 51,\n",
       " 'company': 52,\n",
       " 'wayne': 53,\n",
       " 'county': 54,\n",
       " 'regional': 55,\n",
       " 'service': 56,\n",
       " 'agency': 57,\n",
       " 'admits': 58,\n",
       " 'located': 59,\n",
       " 'dearborn': 60,\n",
       " 'michigan': 61,\n",
       " 'campus': 62,\n",
       " 'enrollment': 63,\n",
       " 'taken': 64,\n",
       " 'from': 65,\n",
       " 'lottery': 66,\n",
       " 'area': 67,\n",
       " 'totaled': 68,\n",
       " '467': 69,\n",
       " '2010': 70,\n",
       " '1': 71,\n",
       " 'freshman': 72,\n",
       " 'meet': 73,\n",
       " 'inside': 74,\n",
       " 'glass': 75,\n",
       " 'walled': 76,\n",
       " 'while': 77,\n",
       " 'older': 78,\n",
       " 'converted': 79,\n",
       " 'carousel': 80,\n",
       " 'pullman': 81,\n",
       " 'cars': 82,\n",
       " 'siding': 83,\n",
       " 'greenfield': 84,\n",
       " 'railroad': 85,\n",
       " 'classes': 86,\n",
       " 'are': 87,\n",
       " 'expected': 88,\n",
       " 'artifacts': 89,\n",
       " 'tradition': 90,\n",
       " 'when': 91,\n",
       " 'was': 92,\n",
       " 'established': 93,\n",
       " '1929': 94,\n",
       " 'included': 95,\n",
       " 'which': 96,\n",
       " 'served': 97,\n",
       " 'grades': 98,\n",
       " 'kindergarten': 99,\n",
       " 'college': 100,\n",
       " 'trade': 101,\n",
       " 'ages': 102,\n",
       " 'last': 103,\n",
       " 'part': 104,\n",
       " 'closed': 105,\n",
       " '1969': 106,\n",
       " 'learning': 107,\n",
       " 'institute': 108,\n",
       " 'using': 109,\n",
       " 'model': 110,\n",
       " 'further': 111,\n",
       " 'including': 112,\n",
       " 'power': 113,\n",
       " 'house': 114,\n",
       " 'chicago': 115,\n",
       " 'alameda': 116,\n",
       " 'art': 117,\n",
       " 'san': 118,\n",
       " 'antonio': 119,\n",
       " 'received': 120,\n",
       " 'annual': 121,\n",
       " 'council': 122,\n",
       " 'planners': 123,\n",
       " '2001': 124,\n",
       " 'james': 125,\n",
       " 'd': 126,\n",
       " 'macconnell': 127,\n",
       " 'outstanding': 128,\n",
       " 'new': 129,\n",
       " 'notable': 130,\n",
       " 'attendees': 131,\n",
       " 'chris': 132,\n",
       " 'stroud': 133,\n",
       " 'isaac': 134,\n",
       " 'sudut': 135,\n",
       " 'see': 136,\n",
       " 'also': 137,\n",
       " 'edit': 138}"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_ex_li.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('the', 25),\n",
       "             ('main', 2),\n",
       "             ('henry', 7),\n",
       "             ('ford', 8),\n",
       "             ('museum', 6),\n",
       "             ('building', 4),\n",
       "             ('houses', 1),\n",
       "             ('some', 1),\n",
       "             ('of', 8),\n",
       "             ('classrooms', 2),\n",
       "             ('for', 5),\n",
       "             ('academy', 3),\n",
       "             ('is', 5),\n",
       "             ('first', 1),\n",
       "             ('charter', 2),\n",
       "             ('school', 7),\n",
       "             ('in', 9),\n",
       "             ('united', 1),\n",
       "             ('states', 1),\n",
       "             ('to', 3),\n",
       "             ('be', 1),\n",
       "             ('developed', 1),\n",
       "             ('jointly', 1),\n",
       "             ('by', 2),\n",
       "             ('a', 7),\n",
       "             ('global', 1),\n",
       "             ('corporation', 1),\n",
       "             ('public', 1),\n",
       "             ('education', 1),\n",
       "             ('and', 7),\n",
       "             ('major', 1),\n",
       "             ('nonprofit', 1),\n",
       "             ('cultural', 1),\n",
       "             ('institution', 1),\n",
       "             ('sponsored', 1),\n",
       "             ('motor', 1),\n",
       "             ('company', 1),\n",
       "             ('wayne', 1),\n",
       "             ('county', 1),\n",
       "             ('regional', 1),\n",
       "             ('educational', 3),\n",
       "             ('service', 1),\n",
       "             ('agency', 1),\n",
       "             ('admits', 1),\n",
       "             ('high', 2),\n",
       "             ('students', 2),\n",
       "             ('it', 2),\n",
       "             ('located', 1),\n",
       "             ('dearborn', 1),\n",
       "             ('michigan', 1),\n",
       "             ('on', 2),\n",
       "             ('campus', 1),\n",
       "             ('enrollment', 1),\n",
       "             ('taken', 1),\n",
       "             ('from', 1),\n",
       "             ('lottery', 1),\n",
       "             ('area', 1),\n",
       "             ('totaled', 1),\n",
       "             ('467', 1),\n",
       "             ('2010', 1),\n",
       "             ('1', 1),\n",
       "             ('freshman', 1),\n",
       "             ('meet', 1),\n",
       "             ('inside', 1),\n",
       "             ('glass', 1),\n",
       "             ('walled', 1),\n",
       "             ('while', 1),\n",
       "             ('older', 1),\n",
       "             ('use', 2),\n",
       "             ('converted', 1),\n",
       "             ('carousel', 1),\n",
       "             ('pullman', 1),\n",
       "             ('cars', 1),\n",
       "             ('siding', 1),\n",
       "             ('greenfield', 1),\n",
       "             ('village', 2),\n",
       "             ('railroad', 1),\n",
       "             ('classes', 1),\n",
       "             ('are', 1),\n",
       "             ('expected', 1),\n",
       "             ('include', 2),\n",
       "             ('artifacts', 1),\n",
       "             ('tradition', 1),\n",
       "             ('original', 2),\n",
       "             ('schools', 2),\n",
       "             ('when', 1),\n",
       "             ('was', 1),\n",
       "             ('established', 1),\n",
       "             ('1929', 1),\n",
       "             ('included', 1),\n",
       "             ('which', 1),\n",
       "             ('served', 1),\n",
       "             ('grades', 1),\n",
       "             ('kindergarten', 1),\n",
       "             ('college', 1),\n",
       "             ('trade', 1),\n",
       "             ('ages', 1),\n",
       "             ('last', 1),\n",
       "             ('part', 1),\n",
       "             ('closed', 1),\n",
       "             ('1969', 1),\n",
       "             ('learning', 1),\n",
       "             ('institute', 1),\n",
       "             ('using', 1),\n",
       "             ('model', 1),\n",
       "             ('further', 1),\n",
       "             ('including', 1),\n",
       "             ('power', 1),\n",
       "             ('house', 1),\n",
       "             ('chicago', 1),\n",
       "             ('alameda', 1),\n",
       "             ('art', 1),\n",
       "             ('design', 2),\n",
       "             ('san', 1),\n",
       "             ('antonio', 1),\n",
       "             ('received', 1),\n",
       "             ('international', 2),\n",
       "             ('annual', 1),\n",
       "             ('award', 2),\n",
       "             ('council', 1),\n",
       "             ('facilities', 2),\n",
       "             ('planners', 1),\n",
       "             ('2001', 1),\n",
       "             ('james', 1),\n",
       "             ('d', 1),\n",
       "             ('macconnell', 1),\n",
       "             ('outstanding', 1),\n",
       "             ('new', 1),\n",
       "             ('notable', 1),\n",
       "             ('attendees', 1),\n",
       "             ('chris', 1),\n",
       "             ('stroud', 1),\n",
       "             ('isaac', 1),\n",
       "             ('sudut', 1),\n",
       "             ('see', 1),\n",
       "             ('also', 1),\n",
       "             ('edit', 1)])"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_ex_li.word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_ex_li.word_counts['school']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- tokenizer_ex_li.word_counts['oov']를 치면 오류가 나옴\n",
    "- 즉, tokenizer_ex_li.word_counts안에는 'oov'라는 키는 없다!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "exam_seq=tokenizer_ex_li.texts_to_sequences(example_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 16, 5, 3, 9, 12, 33, 34, 4, 1, 17, 10, 1, 5, 3, 13], [5, 3, 13, 11, 1, 35, 18, 6, 2, 1, 36, 37, 14, 38, 39, 40, 19, 7, 41, 42, 43, 44, 8, 7, 45, 46, 47, 48, 1, 6, 11, 49, 19, 1, 3, 50, 51, 52, 53, 54, 15, 55, 56, 8, 1, 5, 3, 9, 8, 57, 20, 6, 21, 22, 11, 58, 2, 59, 60, 23, 1, 61, 4, 1, 5, 3, 9, 62, 11, 63, 64, 7, 65, 2, 1, 66, 8, 67, 68, 2, 69, 70], [71, 72, 73, 1, 16, 9, 12, 2, 74, 75, 17, 76, 77, 21, 24, 7, 78, 79, 12, 8, 80, 81, 23, 7, 82, 4, 1, 83, 25, 84, 85, 86, 87, 14, 26, 24, 4, 1, 9, 88, 7, 89, 4, 1, 27, 25, 28, 90, 1, 9, 91, 92, 2, 93, 22, 94, 7, 6, 95, 96, 97, 98, 14, 99, 100, 6, 101, 1, 102, 103, 4, 1, 27, 6, 104, 2, 105], [1, 5, 3, 106, 107, 11, 108, 1, 5, 3, 13, 109, 10, 110, 18, 28, 111, 1, 112, 113, 20, 2, 114, 8, 115, 6, 10, 116, 29, 2, 117, 118], [1, 12, 119, 1, 30, 120, 29, 31, 4, 1, 121, 4, 15, 32, 122, 30, 10, 123, 1, 124, 125, 126, 31, 10, 127, 128, 15, 32, 129, 130, 26, 131, 132, 8, 133, 134], [135, 136, 137]]\n"
     ]
    }
   ],
   "source": [
    "print(exam_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "exam_ohe=[to_categorical(exam_seq[i]) for i in range(len(exam_seq))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [2] 하나의 문자열로 만들어 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_ex=Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_ex.fit_on_texts(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'e': 1,\n",
       " 'a': 2,\n",
       " 'o': 3,\n",
       " 'n': 4,\n",
       " 'i': 5,\n",
       " 't': 6,\n",
       " 's': 7,\n",
       " 'r': 8,\n",
       " 'l': 9,\n",
       " 'd': 10,\n",
       " 'h': 11,\n",
       " 'c': 12,\n",
       " 'u': 13,\n",
       " 'm': 14,\n",
       " 'f': 15,\n",
       " 'g': 16,\n",
       " 'y': 17,\n",
       " 'b': 18,\n",
       " 'p': 19,\n",
       " 'w': 20,\n",
       " 'v': 21,\n",
       " '1': 22,\n",
       " '0': 23,\n",
       " '9': 24,\n",
       " 'j': 25,\n",
       " '2': 26,\n",
       " 'k': 27,\n",
       " '6': 28,\n",
       " '4': 29,\n",
       " '7': 30,\n",
       " 'x': 31}"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_ex.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 단어 기반 인코딩 \n",
    "- Tokenizer : 문장으로부터 단어를 토큰화하고 숫자에 대응시키는 딕셔너리를 사용할 수 있도록\n",
    "\n",
    "- fit_on_texts() 메서드 => 문자 데이터를 입력받아서 리스트의 형태로 변환\n",
    "- texts_to_sequences() => 텍스트를 시퀀스로 변환하기\n",
    "- oov_token => 토큰화되지 않은 단어 처리하기\n",
    "- word_index 속성 => 단어와 숫자의 키-값 쌍을 포함하는 딕셔너리를 반환\n",
    "    - 구두점(,!?)은 인코딩에 영향을 주지 않음, 대문자-소문자 변환"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 패딩 설정하기 =>  pad_sequences() : 서로 다른 개수의 단어로 이루어진 문장을 같은 길이로 만들어주기 위해, 길이 기준 설정\n",
    "    - 긴 경우, 짧은 경우 => 앞/뒤 중 선택\n",
    "    - 값 => 패딩에 들어갈 값\n",
    "        - 파라미터 padding을 ‘post’로 지정하면 시퀀스의 뒤에 패딩이 채워짐. 디폴트는 ‘pre’\n",
    "        - maxlen 파라미터는 시퀀스의 최대 길이를 제한, 넘는 시퀸스는 잘라냄."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- sequences는 정수의 시퀀스로 변환된 텍스트 문장.\n",
    "- pad_sequences 함수에 이 시퀀스를 입력하면 숫자 0을 이용해서 같은 길이의 시퀀스로 변환\n",
    "- 가장 긴 시퀀스의 길이를 기준으로 모두 같은 길이의 시퀀스를 포함하는 NumPy 어레이로 변환함."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "d1dde8d3f1fc6169eb2afb9c884f1482ff31994a855398e316a83a9dc8ff488b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
